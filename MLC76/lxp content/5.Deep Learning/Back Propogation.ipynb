{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75add8f5-9f6f-4e24-927f-5d555856af9f",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "\n",
    "In the previous session, you learnt that training refers to the task of finding the optimal combination of weights and biases to minimize the total loss (with a fixed set of hyperparameters).  \n",
    "\n",
    "\n",
    "This optimization is achieved using the familiar gradient descent algorithm.\n",
    "\n",
    " \n",
    "\n",
    "For a neural network, you will learn how the loss function is minimized using the gradient descent function by finding the optimum values of weights and biases using backpropagation.\n",
    "\n",
    "<div class=\"text_component\" data-testid=\"text-component\"><p>In this video, you learnt that the gradient descent algorithm presents us with the following parameter update equation:</p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"w_{kj}^{l}=w_{kj}^{l}-\\eta \\frac{\\partial L}{\\partial w_{kj}^{l}}\" src=\"https://latex.upgrad.com/render?formula=w_%7Bkj%7D%5E%7Bl%7D%3Dw_%7Bkj%7D%5E%7Bl%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7Bkj%7D%5E%7Bl%7D%7D\"></p><p>where&nbsp;<img alt=\"Equation\" data-latex=\"k\" src=\"https://latex.upgrad.com/render?formula=k\" style=\"vertical-align: middle;display: inline;\"> and&nbsp;<img alt=\"Equation\" data-latex=\"j\" src=\"https://latex.upgrad.com/render?formula=j\" style=\"vertical-align: middle;display: inline;\"> are the indices of the weight in the weight matrix and&nbsp;<img alt=\"Equation\" data-latex=\"l\" src=\"https://latex.upgrad.com/render?formula=l\" style=\"vertical-align: middle;display: inline;\"> is the index of the layer to which it belongs.</p><p><img alt=\"Backpropagation\" src=\"https://images.upgrad.com/dd815728-a334-4a24-9573-08aa555fe008-forward%20pass.png\"></p><p>Given the neural network in the diagram, for the <strong>output</strong> layer, the following weights and bias terms will be updated using the gradient descent update equation:</p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"w_{11}^{2}=w_{11}^{2}-\\eta \\frac{\\partial L}{\\partial w_{11}^{2}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B11%7D%5E%7B2%7D%3Dw_%7B11%7D%5E%7B2%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B11%7D%5E%7B2%7D%7D\"></p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"w_{12}^{2}=w_{12}^{2}-\\eta \\frac{\\partial L}{\\partial w_{12}^{2}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B12%7D%5E%7B2%7D%3Dw_%7B12%7D%5E%7B2%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B12%7D%5E%7B2%7D%7D\"></p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"b_{1}^{2}=b_{1}^{2}-\\eta \\frac{\\partial L}{\\partial b_{1}^{2}}\" src=\"https://latex.upgrad.com/render?formula=b_%7B1%7D%5E%7B2%7D%3Db_%7B1%7D%5E%7B2%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20b_%7B1%7D%5E%7B2%7D%7D\"></p><p>Now, for the <strong>hidden</strong> layer, the following weight and biases will be updated:</p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"w_{11}^{1}=w_{11}^{1}-\\eta \\frac{\\partial L}{\\partial w_{11}^{1}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B11%7D%5E%7B1%7D%3Dw_%7B11%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B11%7D%5E%7B1%7D%7D\">&nbsp; &nbsp; &nbsp;<img alt=\"Equation\" data-latex=\"w_{21}^{1}=w_{21}^{1}-\\eta \\frac{\\partial L}{\\partial w_{21}^{1}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B21%7D%5E%7B1%7D%3Dw_%7B21%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B21%7D%5E%7B1%7D%7D\"></p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"w_{12}^{1}=w_{12}^{1}-\\eta \\frac{\\partial L}{\\partial w_{12}^{1}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B12%7D%5E%7B1%7D%3Dw_%7B12%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B12%7D%5E%7B1%7D%7D\">&nbsp; &nbsp;<img alt=\"Equation\" data-latex=\"w_{22}^{1}=w_{22}^{1}-\\eta \\frac{\\partial L}{\\partial w_{22}^{1}}\" src=\"https://latex.upgrad.com/render?formula=w_%7B22%7D%5E%7B1%7D%3Dw_%7B22%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B22%7D%5E%7B1%7D%7D\"></p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"b_{1}^{1}=b_{1}^{1}-\\eta \\frac{\\partial L}{\\partial b_{1}^{1}}\" src=\"https://latex.upgrad.com/render?formula=b_%7B1%7D%5E%7B1%7D%3Db_%7B1%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20b_%7B1%7D%5E%7B1%7D%7D\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt=\"Equation\" data-latex=\"b_{2}^{1}=b_{2}^{1}-\\eta \\frac{\\partial L}{\\partial b_{2}^{1}}\" src=\"https://latex.upgrad.com/render?formula=b_%7B2%7D%5E%7B1%7D%3Db_%7B2%7D%5E%7B1%7D-%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20b_%7B2%7D%5E%7B1%7D%7D\"></p><p>To compute these gradients, we use an algorithm called <strong>backpropagation</strong>.</p><p><br>As you can see in the formulas above, there exist partial derivatives of the loss function&nbsp;<img alt=\"Equation\" data-latex=\"L\" src=\"https://latex.upgrad.com/render?formula=L\" style=\"vertical-align: middle;display: inline;\"> with respect to the weights and biases in these equations given above. To compute these, we use the chain rule; you can observe the dependencies of different layers in the gradient computation.</p><p>Now, let’s simplify the neural network given above and represent it in a condensed format - this is shown below.</p><p><img data-height=\"330\" data-width=\"538\" height=\"330\" src=\"https://images.upgrad.com/270e5272-e7fa-48c1-b318-a5ff22078c6b-chain_rule.png\" width=\"538\"></p><p>In this case, the loss function is a function of <img alt=\"Equation\" data-latex=\"w_{1}\" src=\"https://latex.upgrad.com/render?formula=w_%7B1%7D\" style=\"vertical-align: middle;display: inline;\">, <img alt=\"Equation\" data-latex=\"b_{1}\" src=\"https://latex.upgrad.com/render?formula=b_%7B1%7D\" style=\"vertical-align: middle;display: inline;\">, <img alt=\"Equation\" data-latex=\"w_{2}\" src=\"https://latex.upgrad.com/render?formula=w_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">&nbsp;and <img alt=\"Equation\" data-latex=\"b_{2}\" src=\"https://latex.upgrad.com/render?formula=b_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">.</p><p>The loss function, the activation function and the cumulative input are shown in the following expressions:</p><p>Loss function:&nbsp;<img alt=\"Equation\" data-latex=\"Loss = \\frac{1}{2}(y-h_{2})^{2}\" src=\"https://latex.upgrad.com/render?formula=Loss%20%3D%20%5Cfrac%7B1%7D%7B2%7D%28y-h_%7B2%7D%29%5E%7B2%7D\" style=\"vertical-align: middle;display: inline;\"></p><p>Cumulative Input:<b>&nbsp;<b><img alt=\"Equation\" data-latex=\"z_{i}=w_{i}h_{i-1}+b_{i}\" src=\"https://latex.upgrad.com/render?formula=z_%7Bi%7D%3Dw_%7Bi%7Dh_%7Bi-1%7D%2Bb_%7Bi%7D\" style=\"vertical-align: middle;display: inline;\"></b></b></p><p>Output using tanh activation function:&nbsp;<img alt=\"Equation\" data-latex=\"h_{i}=tanh(z_{i})\" src=\"https://latex.upgrad.com/render?formula=h_%7Bi%7D%3Dtanh%28z_%7Bi%7D%29\" style=\"vertical-align: middle;display: inline;\"></p><p>Now, let’s compute the gradient of the loss function with respect to one of the weights to understand how <strong>backpropagation</strong> works.&nbsp;Suppose we want to calculate <img alt=\"Equation\" data-latex=\"\\frac{\\partial L}{\\partial w_{2}}\" src=\"https://latex.upgrad.com/render?formula=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B2%7D%7D\" style=\"vertical-align: middle;display: inline;\">:</p><p><img data-height=\"434\" data-width=\"1068\" height=\"243.82022471910113\" src=\"https://images.upgrad.com/c93418e9-de20-4c7a-84ad-4ef2ce5ae73e-Screenshot 2022-02-13 at 5.41.24 PM.png\" width=\"600\"></p><p><br>Using the chain rule, we can say that:</p><p style=\"text-align: center;\"><img alt=\"Equation\" data-latex=\"\\frac{\\partial L}{\\partial w_{2}}= \\frac{\\partial L}{\\partial h_{2}}\\frac{\\partial h_{2}}{\\partial z_{2}}\\frac{\\partial z_{2}}{\\partial w_{2}}\" src=\"https://latex.upgrad.com/render?formula=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w_%7B2%7D%7D%3D%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20h_%7B2%7D%7D%5Cfrac%7B%5Cpartial%20h_%7B2%7D%7D%7B%5Cpartial%20z_%7B2%7D%7D%5Cfrac%7B%5Cpartial%20z_%7B2%7D%7D%7B%5Cpartial%20w_%7B2%7D%7D\"></p><p>Based on the definition of the loss function,&nbsp;<img alt=\"Equation\" data-latex=\"L\" src=\"https://latex.upgrad.com/render?formula=L\" style=\"vertical-align: middle;display: inline;\"> is a direct function of <img alt=\"Equation\" data-latex=\"h_{2}\" src=\"https://latex.upgrad.com/render?formula=h_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">, <img alt=\"Equation\" data-latex=\"h_{2}\" src=\"https://latex.upgrad.com/render?formula=h_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">&nbsp;is a direct function of&nbsp;<img alt=\"Equation\" data-latex=\"z_{2}\" src=\"https://latex.upgrad.com/render?formula=z_%7B2%7D\" style=\"vertical-align: middle;display: inline;\"> and <img alt=\"Equation\" data-latex=\"z_{2}\" src=\"https://latex.upgrad.com/render?formula=z_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">&nbsp;is a direct function of <img alt=\"Equation\" data-latex=\"w_{2}\" src=\"https://latex.upgrad.com/render?formula=w_%7B2%7D\" style=\"vertical-align: middle;display: inline;\">. Now, in the following video, let’s calculate all of them one by one.</p></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a4379-61da-456a-9610-7bfbcf744c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

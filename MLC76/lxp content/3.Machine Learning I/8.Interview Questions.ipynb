{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a968fd87-f493-49b7-86c4-62e5a985527b",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "\n",
    "<div class=\"MuiBox-root css-j7qwjs\"><div class=\"MuiBox-root css-lrle2m-container\"><div class=\"text_component\"><p style=\"text-align: justify;\"><strong><span style=\"font-size: 14px;\">Subjective Questions - I<br></span></strong></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><br>It is a common practice to test data science aspirants on linear regression as it is the first algorithm that almost everyone studies in Data Science/Machine Learning. Aspirants are expected to possess an in-depth knowledge of these algorithms. We consulted hiring managers and data scientists from various organisations to know about the typical Linear Regression questions which they ask in an interview. Based on their extensive feedback a set of question and answers were prepared to help students in their conversations.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q1. What is linear regression?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q2. What are assumptions in a linear regression model?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The assumptions of linear regression are:</span></p><ol><li style=\"font-size: 14px; text-align: justify;\">The assumption about the form of the model:&nbsp;It is assumed that there is a linear relationship between the dependent and independent variables. It is known as the ‘linearity assumption’.</li><li style=\"font-size: 14px;\">Assumptions about the residuals:<ol style=\"font-size: initial;\"><li style=\"font-size: 14px; text-align: justify;\">Normality assumption: It is assumed that the error terms,&nbsp;ε<sup>(i)</sup>, are normally distributed.</li><li style=\"font-size: 14px; text-align: justify;\">Zero mean assumption: It is assumed that the residuals have a mean value of zero, i.e., the error terms are normally distributed around zero.</li><li style=\"font-size: 14px; text-align: justify;\">Constant variance assumption: It is assumed that the residual terms have the same (but unknown) variance, σ<sup>2</sup> . This assumption is also known as the assumption of homogeneity or homoscedasticity.</li><li style=\"font-size: 14px; text-align: justify;\">Independent error assumption: It is assumed that the residual terms are independent of each other, i.e. their pair-wise covariance is zero.</li></ol></li><li style=\"font-size: 14px;\">Assumptions about the estimators:<ol style=\"font-size: initial;\"><li style=\"font-size: 14px; text-align: justify;\">The independent variables are measured without error.</li><li style=\"font-size: 14px; text-align: justify;\">The independent variables are linearly independent of each other, i.e. there is no multicollinearity in the data.</li></ol></li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><em>Explanations:</em></span></p><ol><li style=\"font-size: 14px; text-align: justify;\">This is self-explanatory.</li><li style=\"font-size: 14px; text-align: justify;\">If the residuals are not normally distributed, their randomness is lost, which implies that the model is not able to explain the relation in the data.<br>Also, the mean of the residuals should be zero.<br>Y<sup>(i)i</sup>= β<sub>0</sub>+ β<sub>1</sub>x<sup>(i)</sup> + ε<sup>(i)</sup><br>This is the assumed linear model, where ε is the residual term.<br>E(Y) = E(β<sub>0</sub>+ β<sub>1</sub>x<sup>(i)</sup> + ε<sup>(i)</sup>)<br>= E(β<sub>0</sub>+ β<sub>1</sub>x<sup>(i)</sup> + ε<sup>(i)</sup>)<br>If the expectation(mean) of residuals, E(ε<sup>(i)</sup>), is zero, the expectations of the target variable and the model become the same, which is one of the targets of the model.<br>The residuals (also known as error terms) should be independent. This means that there is no correlation between the residuals and the predicted values, or among the residuals themselves. If some correlation is present, it implies that there is some relation that the regression model is not able to identify.</li><li style=\"font-size: 14px; text-align: justify;\">If the independent variables are not linearly independent of each other, the uniqueness of the least squares solution (or normal equation solution) is lost.</li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q3.&nbsp;What is heteroscedasticity? What are the consequences, and how can you overcome it?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">A random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation).</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">One of the basic assumptions of linear regression is that the data should be homoscedastic, i.e., heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the <a href=\"https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\">Best Linear Unbiased Estimators (BLUE)</a>. Hence, they do not give the&nbsp;least variance than other Linear Unbiased Estimators (LUEs).</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">There is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are —</span></p><ol><li style=\"font-size: 14px; text-align: justify;\">Logarithmising the data: A series that is increasing exponentially often results in increased variability. This can be overcome using the log transformation.</li><li style=\"font-size: 14px; text-align: justify;\">Using weighted linear regression: Here, the OLS method is applied to the weighted values of X and Y. One way is to attach weights directly related to the magnitude of the dependent variable.</li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q4.&nbsp;How do you know that linear regression is suitable for any given data?</strong><br>To see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in the case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q5.&nbsp;How is hypothesis testing used in linear regression?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Hypothesis testing can be carried out in linear regression for the following purposes:</span></p><ol><li style=\"font-size: 14px;\">To check whether a predictor is significant for the prediction of the target variable. Two common methods for this are —<ol style=\"font-size: initial;\"><li style=\"font-size: 14px; text-align: justify;\">By the use of p-values:<br>If the p-value of a variable is greater than a certain limit (usually 0.05), the variable is insignificant in the prediction of the target variable.</li><li style=\"font-size: 14px; text-align: justify;\">By checking the values of the regression coefficient:<br>If the value of the regression coefficient corresponding to a predictor is zero, that variable is insignificant in the prediction of the target variable and has no linear relationship with it.</li></ol></li><li style=\"font-size: 14px; text-align: justify;\">To check whether the calculated regression coefficients are good estimators of the actual coefficients.</li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The Null and Alternate Hypothesis&nbsp;used in the case of linear regression, respectively, are:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><span tabindex=\"-1\"><span tabindex=\"-1\"><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"10\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"\\beta_1 = 0\" src=\"https://latex.upgrad.com/render?formula=%5Cbeta_1%20%3D%200\" style=\"vertical-align: middle;display: inline;\"></span></span></span></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><span tabindex=\"-1\"><span tabindex=\"-1\"><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"9\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"\\beta_1 \\neq 0\" src=\"https://latex.upgrad.com/render?formula=%5Cbeta_1%20%5Cneq%200\" style=\"vertical-align: middle;display: inline;\"></span></span></span></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Thus, if we reject the Null hypothesis, we can say that the coefficient <span tabindex=\"-1\"><span tabindex=\"-1\"><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"8\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"\\beta_1\" src=\"https://latex.upgrad.com/render?formula=%5Cbeta_1\" style=\"vertical-align: middle;display: inline;\"></span></span></span> is not equal to zero and hence, is significant for the model. On the other hand, if we fail to reject the Null hypothesis, it is concluded that the coefficient is insignificant and should be dropped from the model.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q6.&nbsp;How do you interpret a linear regression model?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">A linear regression model is quite easy to interpret. The model is of the following form:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span tabindex=\"-1\"><span tabindex=\"-1\"><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"7\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...+ \\beta_nX_n\" src=\"https://latex.upgrad.com/render?formula=y%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1X_1%20%2B%20%5Cbeta_2X_2%20%2B%20...%2B%20%5Cbeta_nX_n\" style=\"vertical-align: middle;display: inline;\"></span></span></span></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of <em>x<sub>0</sub></em> increases by 1 unit, keeping other variables constant, the total increase in the value of <em>y</em> will be <em>β<sub>i</sub></em>. Mathematically, the intercept term (<em>β<sub>0</sub></em>) is the response when all the predictor terms are set to zero or not considered.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q7. What are the shortcomings of linear regression?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">You should never just run a regression without having a good look at your data because simple linear regression has quite a few shortcomings:</span></p><ol><li style=\"font-size: 14px; text-align: justify;\">It is sensitive to outliers</li><li style=\"font-size: 14px; text-align: justify;\">It models the linear relationships only</li><li style=\"font-size: 14px; text-align: justify;\">A few assumptions are required to make the inference</li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">These <strong>phenomena</strong> can be best explained by the Anscombe's Quartet, shown below:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><img acl=\"public-read\" bucket=\"upgrad-prod\" class=\"image-editor fr-fic fr-dib\" contenttype=\"image/png\" data-height=\"270\" encoding=\"7bit\" etag=\"&quot;251fa951fba214316197bf76824d7de9&quot;\" fieldname=\"upload\" key=\"141b3bc8-5205-4955-85a2-013cccf8c6df-1741002303501.png\" location=\"https://d35ev2v1xsdze0.cloudfront.net/141b3bc8-5205-4955-85a2-013cccf8c6df-1741002303501.png\" mimetype=\"image/png\" originalname=\"1741002303501.png\" serversideencryption=\"AES256\" size=\"24171\" src=\"https://d35ev2v1xsdze0.cloudfront.net/141b3bc8-5205-4955-85a2-013cccf8c6df-1741002303501.png\" style=\"width: 500px;\"></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">As we can see, all the four linear regression are exactly the same. But there are some peculiarities in the datasets which have fooled the regression line. While the first one seems to be doing a decent job, the second one clearly shows that linear regression can only model linear relationships and is incapable of handling any other kind of data. The third and fourth images&nbsp;showcase the linear regression model's sensitivity to outliers. Had the outlier not been present, we could have gotten a great line fitted through the data points. So we should never ever run a regression without having a good look at our data.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q8. What parameters are used to check the significance of the model and the goodness of fit?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">To check if the overall model fit is significant or not, the primary parameter to be looked at is the <strong>F-statistic</strong>. While the t-test along with the p-values for betas test if each coefficient is significant or not individually, the F-statistic is a measure that can determine whether the overall model fit with all the coefficients is significant or not.&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The basic idea behind the F-test is that it is a relative comparison between the model that you've built and the model without any of the coefficients except for <span tabindex=\"-1\"><span tabindex=\"-1\"><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"6\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"\\beta_0\" src=\"https://latex.upgrad.com/render?formula=%5Cbeta_0\" style=\"vertical-align: middle;display: inline;\"></span></span></span>. If the value of the F-statistic is high, it would mean that the Prob(F) would be low and hence, you can conclude that the model is significant. On the other hand, if the value of F-statistic is low, it might lead to the value of Prob(F) being higher&nbsp;than the significance level (taken 0.05, usually) which in turn would conclude that the overall model fit is insignificant and the intercept-only model can provide a better fit.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Apart from that, to test the goodness or the extent of fit, we look at a parameter called <strong>R-squared</strong> (for simple linear regression models) or <strong>Adjusted R-squared</strong> (for multiple linear regression models). If your overall model fit is deemed to be significant by the F-test, you can go ahead and look at the value of R-squared. This value lies between 0 and 1, with 1 meaning a perfect fit. A higher value of R-squared is indicative of the model being good with much of the variance in the data being explained by the straight line fitted. For example, an R-squared value of 0.75 means that 75% of the variance in the data is being explained by the model. But it is important to remember than R-squared only tells the extent of the fit and should not be used to determine whether the model fit is significant or not.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q9.&nbsp;</strong><strong>If two variables are correlated, is it necessary that they have a linear relationship?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">No, not necessarily. If two variables are correlated, it is very much possible that they have some other sort of relationship and not just a linear one.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">But the important point to note here is that there are two correlation coefficients that are widely used in regression. One is the Pearson's R correlation coefficient which is the correlation coefficient you've studied in the linear regression model. This correlation coefficient is designed for linear relationships and it might not be a good measure for if the relationship between the variables is non-linear. The other correlation coefficient is Spearman's R which is used to determine the correlation if the relationship between the variables is not linear. So even though, Pearson's R might give a correlation coefficient for non-linear relationships, it might not be reliable. For example, the correlation coefficients as given by both the techniques for the relationship </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"5\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"y = X^3\" src=\"https://latex.upgrad.com/render?formula=y%20%3D%20X%5E3\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\"> for 100 equally separated values between 1 and 100 were found out to be:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"4\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"Pearson's \\space R \\approx &nbsp;0.91\" src=\"https://latex.upgrad.com/render?formula=Pearson%27s%20%5Cspace%20R%20%5Capprox%20%C2%A00.91\" style=\"vertical-align: middle;display: inline;\"></span></p><p style=\"text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"3\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"Spearman's \\space R \\approx 1\" src=\"https://latex.upgrad.com/render?formula=Spearman%27s%20%5Cspace%20R%20%5Capprox%201\" style=\"vertical-align: middle;display: inline;\"></span></p><p style=\"text-align: justify;\">And as we keep on increasing the power, the Pearson's R value consistently drop whereas the Spearman's R remains robust at 1. For example, for the relationship <span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"2\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"y = X^{10}\" src=\"https://latex.upgrad.com/render?formula=y%20%3D%20X%5E%7B10%7D\" style=\"vertical-align: middle;display: inline;\"></span> for the same data points, the coefficients were:</p><p style=\"text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"1\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"Pearson's \\space R \\approx 0.66\" src=\"https://latex.upgrad.com/render?formula=Pearson%27s%20%5Cspace%20R%20%5Capprox%200.66\" style=\"vertical-align: middle;display: inline;\"></span></p><p style=\"text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"0\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"Spearman's \\space R \\approx 1\" src=\"https://latex.upgrad.com/render?formula=Spearman%27s%20%5Cspace%20R%20%5Capprox%201\" style=\"vertical-align: middle;display: inline;\"></span></p><p style=\"text-align: justify;\">So the takeaway here is that if you have some sense of the relationship being non-linear, you should look at Spearman's R instead of Pearson's R. It might happen that even for a non-linear relationship, the Pearson's R value might be high, but it is simply not reliable.&nbsp;</p><p style=\"text-align: justify;\"><strong>Q10. What is the difference between Least Square Error and Mean Square Error?</strong></p><p style=\"text-align: justify;\">Least Square Error is the method used to find the best-fit line through a set of data points. It is. The idea behind the least squared error method is to minimize the square of errors between the actual data points and the line fitted.&nbsp;</p><p style=\"text-align: justify;\">Mean Square Error, on the other hand, is used once you have fitted the model and want to evaluate it. So the mean squared error finds out the average of the difference between the actual and predicted values and hence, is a good parameter to compare various models on the same data set.</p><p style=\"text-align: justify;\">Thus, LSE is a method used to minimise the sum of squares and is used during model fitting, and MSE is a metric used to evaluate the model after fitting based on the average squared errors.</p></div></div><div class=\"MuiBox-root css-0\"></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103316de-4329-4721-ba3b-c85bc3f3566c",
   "metadata": {},
   "source": [
    "### Multi Linear Regression\n",
    "\n",
    "<div class=\"MuiBox-root css-j7qwjs\"><div class=\"MuiBox-root css-lrle2m-container\"><div class=\"text_component\"><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Subjective Questions - II</strong><br><br>In the previous segment, you saw some common interview questions asked on linear regression. The questions in that segment were mostly related to the essence of linear regression and focused on general concepts related to linear regression. This section extensively covers the common interview questions asked related to the concepts learnt in multiple linear regression.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q1. What is Multicollinearity? How does it&nbsp;affect the linear regression? How can you deal with it?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">One of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">A highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q2. How can you handle categorical variables present in the dataset?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Many a time it might happen that your dataset has categorical variables that might be a potentially good predictor for the response variable. So handling them right is quite crucial.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">One of the ways to handle categorical data with just two levels is to do a binary mapping of the variables wherein one of the levels will correspond to zero and the other to 1.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Another way of handling categorical variables with few levels is to perform a dummy encoding. The key idea behind dummy encoding is that for a variable with, say, 'N' levels, you create 'N-1' new indicator variables for each of these levels. So for a variable say, 'Relationship' with three levels namely, 'Single', 'In a relationship', and 'Married', you would create a dummy table like the following:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><table align=\"center\" border=\"1\" cellpadding=\"1\" cellspacing=\"1\"><tbody><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Relationship Status</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Single</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In a relationship</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Married</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Single</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">1</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In a relationship</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">1</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Married</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">1</span></div></td></tr></tbody></table><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">But you can clearly see that there is no need of defining <strong>three</strong> different levels. If you drop a level, say 'Single', you would still be able to explain the three levels.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Let's drop the dummy variable 'Single' from the columns and see what the table looks like:<br>&nbsp;</span></p><table align=\"center\" border=\"1\" cellpadding=\"1\" cellspacing=\"1\"><tbody><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Relationship Status</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In a relationship</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Married</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Single</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">In a relationship</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">1</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td></tr><tr><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Married</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">0</span></div></td><td><div style=\"text-align: justify;\"><span style=\"font-size: 14px;\">1</span></div></td></tr></tbody></table><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">If both the dummy variables namely 'In a relationship' and 'Married' are equal to zero, that means that the person is single. If 'In a relationship' is one and 'Married' is zero, that means that the person is in a relationship and finally, if 'In a relationship' is zero and 'Married' is 1, that means that the person is married.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Now, creating dummy variables might be useful when the number of levels in a categorical variable is small, but if a categorical variable has a hundred levels, it is clearly impossible to create 99 new variables. In such cases, grouping the variables might be useful. For example, for the variable “Cities in India”, you can use a geographical grouping, i.e.:</span></p><ul><li style=\"font-size: 14px; text-align: justify;\">Keep the 'n' largest cities, group the rest</li><li style=\"font-size: 14px;\">Geographical hierarchy<ul style=\"font-size: initial;\"><li style=\"font-size: 14px; text-align: justify;\">City &lt; District &lt; State &lt; Zone</li></ul></li><li style=\"font-size: 14px; text-align: justify;\">Group cities with the similar value for the outcome variable</li><li style=\"font-size: 14px; text-align: justify;\">Cluster cities with similar values for the predictor variables</li></ul><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Another way to deal with categorical variables is that you can perform a <a href=\"https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\" target=\"_blank\">One-hot encoding</a> which hasn't been covered in our syllabus.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q3. What is the major difference between R-squared and Adjusted R-squared/Why is it advised to use Adjusted R-squared in case of multiple linear regression?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The major difference between R-squared and Adjusted R-squared is that R-squared doesn't penalise the model for having more number of variables. Thus, if you keep on adding variables to the model, the R-squared will always increase (or remain the same in the case when the value of correlation between that variable and the dependent variable is zero). Thus, R-squared assumes that any variable added to the model will increase the predictive power.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Adjusted R-squared on the other hand, penalises models based on the number of variables present in it. Its formula is given as:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img alt=\"Equation\" data-latex=\"A d j . \\backslash space R^{2} = 1 - \\frac{\\left(\\right. 1 - R^{2} \\left.\\right) \\left(\\right. N - 1 \\left.\\right)}{N - k - 1}\" src=\"https://latex.upgrad.com/render?formula=A%20d%20j%20.%20%5Cbackslash%20space%20R%5E%7B2%7D%20%3D%201%20-%20%5Cfrac%7B%5Cleft%28%5Cright.%201%20-%20R%5E%7B2%7D%20%5Cleft.%5Cright%29%20%5Cleft%28%5Cright.%20N%20-%201%20%5Cleft.%5Cright%29%7D%7BN%20-%20k%20-%201%7D\" style=\"vertical-align: middle; display: inline;\"><br><br></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><em>where 'N' is the number of data points and 'k' is the number&nbsp;</em><em>of</em><em>&nbsp;features</em></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">So if you add a variable and the Adjusted R-squared drops, you can be certain that that variable is insignificant to the model and shouldn't be used. So in the case of multiple linear regression, you should always look at the adjusted R-squared value in order to keep redundant variables out from your regression model.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q4. Explain gradient descent with respect to linear regression.</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Gradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Gradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).<br><br><img acl=\"public-read\" bucket=\"upgrad-prod\" class=\"fr-fic fr-dib\" contenttype=\"image/png\" encoding=\"7bit\" etag=\"&quot;6775fb834d7ed395994bac598d18ab0d&quot;\" fieldname=\"upload\" key=\"1710b3fc-ae0f-4fe5-9b41-db1c5831b891-mk505ry8.png\" location=\"https://d35ev2v1xsdze0.cloudfront.net/1710b3fc-ae0f-4fe5-9b41-db1c5831b891-mk505ry8.png\" mimetype=\"image/png\" originalname=\"mk505ry8.png\" serversideencryption=\"AES256\" size=\"89607\" src=\"https://d35ev2v1xsdze0.cloudfront.net/1710b3fc-ae0f-4fe5-9b41-db1c5831b891-mk505ry8.png\" style=\"width: 500px;\"></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><br></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><br></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><span id=\"cke_bm_16361S\" style=\"display: none;\">&nbsp;</span>&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Mathematically, the aim of gradient descent for linear regression is to find the solution of</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">ArgMin J(Θ<em><sub>0</sub></em>,Θ<sub><em>1</em></sub>), where J(Θ<sub><em>0</em></sub>,Θ<em><sub>1</sub></em>) is the cost function of the linear regression. It is given by — &nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><img acl=\"public-read\" bucket=\"upgrad-prod\" class=\"image-editor fr-fic fr-dib\" contenttype=\"image/png\" data-height=\"133\" encoding=\"7bit\" etag=\"&quot;d3d0b2f2659e65f1404bab0ac02fb46c&quot;\" fieldname=\"upload\" key=\"a30688f3-39fb-43c4-9c6b-fb500420c985-1741002545950.png\" location=\"https://d35ev2v1xsdze0.cloudfront.net/a30688f3-39fb-43c4-9c6b-fb500420c985-1741002545950.png\" mimetype=\"image/png\" originalname=\"1741002545950.png\" serversideencryption=\"AES256\" size=\"12264\" src=\"https://d35ev2v1xsdze0.cloudfront.net/a30688f3-39fb-43c4-9c6b-fb500420c985-1741002545950.png\" style=\"width: 500px;\"></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Here, <em>h</em> is the linear hypothesis model, h=Θ<sub><em>0</em></sub> + Θ<sub>1</sub>x, <em>y</em> is the true output, and <em>m</em> is the number of data points in the training set.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Gradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">The update is:</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Repeat until convergence</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><img acl=\"public-read\" bucket=\"upgrad-prod\" class=\"image-editor fr-fic fr-dib\" contenttype=\"image/png\" data-height=\"81\" encoding=\"7bit\" etag=\"&quot;53bc6d373b9418b90a48f120d42d4e81&quot;\" fieldname=\"upload\" key=\"73234cae-9be1-4237-a1d4-444701f8c978-1741002545713.png\" location=\"https://d35ev2v1xsdze0.cloudfront.net/73234cae-9be1-4237-a1d4-444701f8c978-1741002545713.png\" mimetype=\"image/png\" originalname=\"1741002545713.png\" serversideencryption=\"AES256\" size=\"6667\" src=\"https://d35ev2v1xsdze0.cloudfront.net/73234cae-9be1-4237-a1d4-444701f8c978-1741002545713.png\" style=\"width: 500px;\"></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">To read more about gradient descent, refer to the additional resources on linear regression here (<a href=\"https://learn.upgrad.com/course/3094/segment/26812/160582/493508/2548547\" target=\"_blank\">Link</a>).</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q4.&nbsp;What is VIF? How do you calculate it?</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt=\"Equation\" data-latex=\"v \\left|\\right. F F ; \\frac{1}{1 - R_{i}^{2}}\" src=\"https://latex.upgrad.com/render?formula=v%20%5Cleft%7C%5Cright.%20F%20F%20%3B%20%5Cfrac%7B1%7D%7B1%20-%20R_%7Bi%7D%5E%7B2%7D%7D\" style=\"vertical-align: middle; display: inline;\"></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Here, </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"4\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"VIF_i\" src=\"https://latex.upgrad.com/render?formula=VIF_i\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\">&nbsp; is the value of VIF for the </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"3\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"i^{th}\" src=\"https://latex.upgrad.com/render?formula=i%5E%7Bth%7D\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\"> variable, &nbsp;</span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"2\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"{R_i}^2\" src=\"https://latex.upgrad.com/render?formula=%7BR_i%7D%5E2\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\">&nbsp; is the </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"1\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"R^2\" src=\"https://latex.upgrad.com/render?formula=R%5E2\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\"> value of the model when that variable is regressed against all the other independent variables.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">If the value of VIF is high for a variable, it implies that the </span><span aria-label=\"span widget\" class=\"cke_widget_wrapper cke_widget_inline cke_widget_equation-editor cke_widget_wrapper_equation-content cke_widget_selected\" contenteditable=\"false\" data-cke-display-name=\"span\" data-cke-filter=\"off\" data-cke-widget-id=\"0\" data-cke-widget-wrapper=\"1\" role=\"region\" tabindex=\"-1\"><img alt=\"Equation\" data-latex=\"R^2\" src=\"https://latex.upgrad.com/render?formula=R%5E2\" style=\"vertical-align: middle;display: inline;\"></span><span style=\"font-size: 14px;\"> value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\"><strong>Q5. Explain the bias-variance trade-off.</strong></span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a&nbsp;low bias.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">&nbsp;</span></p><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">There is no escaping&nbsp;the relationship between bias and variance in machine learning.</span></p><ol><li style=\"font-size: 14px; text-align: justify;\">Decreasing the bias increases the variance.</li><li style=\"font-size: 14px; text-align: justify;\">Decreasing the variance increases the bias.</li></ol><p style=\"text-align: justify;\"><span style=\"font-size: 14px;\">So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.</span></p></div></div><div class=\"MuiBox-root css-0\"></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f008d-7e42-467a-8e40-33f96ee58229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
